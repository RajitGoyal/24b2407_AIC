{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fjKuldAxwl8"
      },
      "source": [
        "# Q3: Retrieval-Augmented Generation (RAG) over PDF\n",
        "\n",
        "This section demonstrates a RAG chatbot over a PDF using open-source LLMs. Bonus features include history-aware responses, a knowledge graph, KV-cache, and agentic architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4hvLn0pyIL7",
        "outputId": "0065001a-6e21-40c9-a0ab-1b375ab371eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.5.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.60)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.42)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv pypdf langchain-community groq faiss-cpu networkx matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Fugiqwyrjh"
      },
      "source": [
        "## 1. Set Up and Imports\n",
        "\n",
        "We set up the environment and import all necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FAQUhdxkywCv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from groq import Groq\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfgg9GW-yy_y"
      },
      "source": [
        "## 2. API Key Setup\n",
        "\n",
        "Seting  Groq API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TshN9fky8tK"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"fill_your_key_here\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyymc7hmzHya"
      },
      "source": [
        "## 3. PDF Parsing and Chunking\n",
        "\n",
        "Parsing the PDF and spliting it into semantic chunks for retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Z1mIV8AzJuV"
      },
      "outputs": [],
      "source": [
        "def parse_pdf_to_chunks(pdf_path, chunk_size=700, chunk_overlap=100):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = splitter.split_text(text)\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHLEGSDjzYPD"
      },
      "source": [
        "### Vector Store Creation\n",
        "\n",
        "I am using FAISS, a fast vector similarity search library, to build an index of the PDF chunks. Each chunk is embedded using a pre-trained sentence-transformer model. This allows us to efficiently retrieve the most semantically relevant sections of the document for any user query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GNKc7i6Pzb4S"
      },
      "outputs": [],
      "source": [
        "def build_vector_store(chunks):\n",
        "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_texts(chunks, embedding=embedder)\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18qSysBEzxaW"
      },
      "source": [
        "### Retrieval Function\n",
        "\n",
        "Given a user query, we retrieve the top-k most relevant chunks from the vector store using semantic similarity. This ensures that the LLM receives the most contextually appropriate information from the PDF for answer generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cHW531ARz6uA"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_chunks(vector_store, query, k=5):\n",
        "    docs_and_scores = vector_store.similarity_search_with_score(query, k=k)\n",
        "    return [doc.page_content for doc, _ in docs_and_scores]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9znk5ZDvz_4K"
      },
      "source": [
        "### Knowledge Graph Extraction and Visualization\n",
        "\n",
        "We extract simple entity-relation triples from the PDF using regular expressions (e.g., \"X is Y\"). These are used to build a knowledge graph with NetworkX, which helps visualize relationships in the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "STpHd6AL0Mut"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_entities_relations(text):\n",
        "    pattern = r\"(\\w+) (is|has) (\\w+)\"\n",
        "    return re.findall(pattern, text)\n",
        "\n",
        "def build_knowledge_graph(chunks):\n",
        "    G = nx.Graph()\n",
        "    for chunk in chunks:\n",
        "        for entity1, relation, entity2 in extract_entities_relations(chunk):\n",
        "            G.add_edge(entity1, entity2, relation=relation)\n",
        "    return G\n",
        "\n",
        "def show_knowledge_graph(G):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
        "    edge_labels = nx.get_edge_attributes(G, 'relation')\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vcX_ID80U1z"
      },
      "source": [
        "### KV-Cache\n",
        "\n",
        "To speed up repeated queries, we implement a simple key-value cache. If a prompt has already been answered, we return the cached response instead of querying the LLM again. This reduces latency and API usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3GwostUb0a46"
      },
      "outputs": [],
      "source": [
        "response_cache = {}\n",
        "\n",
        "def query_with_cache(prompt, client):\n",
        "    if prompt in response_cache:\n",
        "        return response_cache[prompt]\n",
        "    answer = query_groq(prompt, client)\n",
        "    response_cache[prompt] = answer\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6ovSMG4I014x"
      },
      "outputs": [],
      "source": [
        "class ChatMemory:\n",
        "    def __init__(self):\n",
        "        self.history = []\n",
        "    def add(self, question, answer):\n",
        "        self.history.append((question, answer))\n",
        "    def get_history_str(self):\n",
        "        return \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in self.history])\n",
        "\n",
        "def build_prompt(context_chunks, question, memory):\n",
        "    context_text = '\\n'.join(context_chunks)\n",
        "    history = memory.get_history_str()\n",
        "    return f\"\"\"You are a helpful assistant. Use ONLY the following context and chat history to answer the question.\n",
        "Chat history:\n",
        "{history}\n",
        "\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwLhupjA02rs"
      },
      "source": [
        "### History-Aware Prompt and Memory\n",
        "\n",
        "We maintain a memory of previous questions and answers to provide the LLM with dialogue history. This enables the chatbot to generate more coherent and context-aware multi-turn responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JuHJF-Qf0_d1"
      },
      "outputs": [],
      "source": [
        "class InformationExtractionAgent:\n",
        "    def extract(self, text):\n",
        "        entities = re.findall(r\"\\b[A-Z][a-z]*\\b\", text)\n",
        "        return {\"entities\": entities}\n",
        "\n",
        "class SynthesisAgent:\n",
        "    def synthesize(self, extracted_info):\n",
        "        return f\"Key entities: {', '.join(extracted_info['entities'])}\"\n",
        "\n",
        "class QueryAgent:\n",
        "    def answer(self, question, context, client):\n",
        "        prompt = build_prompt(context, question, memory)\n",
        "        return query_with_cache(prompt, client)\n",
        "\n",
        "class Coordinator:\n",
        "    def __init__(self, client):\n",
        "        self.info_agent = InformationExtractionAgent()\n",
        "        self.synth_agent = SynthesisAgent()\n",
        "        self.query_agent = QueryAgent()\n",
        "        self.client = client\n",
        "    def handle(self, context_chunks, question):\n",
        "        extracted = self.info_agent.extract(\" \".join(context_chunks))\n",
        "        summary = self.synth_agent.synthesize(extracted)\n",
        "        full_context = context_chunks + [summary]\n",
        "        answer = self.query_agent.answer(question, full_context, self.client)\n",
        "        return {\n",
        "            \"extracted\": extracted,\n",
        "            \"summary\": summary,\n",
        "            \"answer\": answer\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PBC5ac01EV5"
      },
      "source": [
        "### Agentic Architecture\n",
        "\n",
        "We modularize the system into agents:\n",
        "- **InformationExtractionAgent:** Extracts entities from context.\n",
        "- **SynthesisAgent:** Summarizes extracted entities.\n",
        "- **QueryAgent:** Handles natural language questions using the RAG pipeline.\n",
        "- **Coordinator:** Orchestrates the workflow between agents and composes the final answer.\n",
        "This architecture makes the system extensible and easier to debug.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pIegREvF1NDl"
      },
      "outputs": [],
      "source": [
        "def query_groq(prompt, client, model=\"llama3-70b-8192\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        max_tokens=512\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IfEaXL1Srg"
      },
      "source": [
        "### Groq LLM Query Function\n",
        "\n",
        "This function sends the constructed prompt to the Groq API (using Llama 3) and returns the generated answer. The LLM is instructed to use only the provided context and chat history for grounded, document-based responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jGnLPRUt1XqT",
        "outputId": "7f5e19d2-b0d3-4718-a07d-908778bdfae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing PDF and building vector store...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-c2bc07f7ebff>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building knowledge graph...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready! Ask questions about the PDF (type 'exit' to quit).\n",
            "\n",
            "Your question: what is the pdf about\n",
            "\n",
            "Answer: The PDF is about the AI Community Assignment 2025, which consists of Technical and Non-Technical problems.\n",
            "\n",
            "Your question: what is the first question\n",
            "\n",
            "Answer: The first question is Technical Problem 1, but the exact question is not specified in the chat history. However, based on the context, it can be inferred that the first question is related to creating a simple communication protocol between agents using JSON, designing a basic coordinator that manages the workflow between agents, and implementing error handling for when agents fail or provide incomplete information.\n",
            "\n",
            "Your question: exit\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    pdf_path = \"/content/AICommunity_Assignment_25.pdf\"  # Change if you want to upload another file\n",
        "    print(\"Parsing PDF and building vector store...\")\n",
        "    chunks = parse_pdf_to_chunks(pdf_path)\n",
        "    vector_store = build_vector_store(chunks)\n",
        "    print(\"Building knowledge graph...\")\n",
        "    G = build_knowledge_graph(chunks)\n",
        "    show_knowledge_graph(G)\n",
        "    print(\"Ready! Ask questions about the PDF (type 'exit' to quit).\")\n",
        "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "    global memory\n",
        "    memory = ChatMemory()\n",
        "    coordinator = Coordinator(client)\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "        context_chunks = retrieve_relevant_chunks(vector_store, question, k=5)\n",
        "        result = coordinator.handle(context_chunks, question)\n",
        "        answer = result[\"answer\"]\n",
        "        print(f\"\\nAnswer: {answer}\")\n",
        "        memory.add(question, answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwEg41Bn1hmr"
      },
      "source": [
        "### Main Chat Loop\n",
        "\n",
        "This is the interactive interface for the RAG chatbot. It initializes all components, displays the knowledge graph, and allows the user to ask questions about the PDF. The system retrieves relevant context, processes it through the agentic pipeline, and generates grounded answers using the Groq LLM.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
